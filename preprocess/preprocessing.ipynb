{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "preprocessing.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqpAXjrxBizU"
      },
      "source": [
        "import re\n",
        "import json\n",
        "import pandas as pd"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_lWEvabIHcs"
      },
      "source": [
        "def _casefold(sentence):\n",
        "    '''\n",
        "    Args:\n",
        "      Input : raw sentence.\n",
        "      Output: lower case sentence.\n",
        "    '''\n",
        "    return str(sentence).lower()"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OhvQ-dHsG02Q"
      },
      "source": [
        "def _normalize(sentence):\n",
        "    '''\n",
        "    Turn sentence into its normal form, as long it's on the dictionary. :)\n",
        "    Args:\n",
        "      Input : lowercase sentence.\n",
        "      Output: normalized sentence. \n",
        "    '''\n",
        "    # replace duplicate chars, e.g okeee to oke\n",
        "    sentence = re.sub(r'(\\w)\\1{2,}', r'\\1\\1', sentence)\n",
        "    # remove \\n and remove space\n",
        "    words = sentence.strip('\\n').split()\n",
        "    \n",
        "    with open('slangwords.json', 'r') as f:\n",
        "        file = json.load(f)\n",
        "        slang = {value:key for value, key in file.items()}\n",
        "    \n",
        "    normal = [slang.get(word, word) for word in sentence.split()]\n",
        "    return ' '.join(normal)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5AaZjx0aXgz"
      },
      "source": [
        "def preprocess(sentence):\n",
        "    '''\n",
        "    Preprocessing the sentence.\n",
        "    Args:\n",
        "      Input : raw sentence.\n",
        "      Output: preprocessed sentence.\n",
        "    '''\n",
        "    sentence = _casefold(sentence)\n",
        "    sentence = _normalize(sentence)\n",
        "    return sentence"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KcZvfsB3IiNt"
      },
      "source": [
        "def to_DF1(df):\n",
        "    '''\n",
        "    Args:\n",
        "      Input : raw dataframe, results of lapor_scraping\n",
        "              query | report | institute | category\n",
        "      Output: dataframe, preprocessed string on `report` column\n",
        "    '''\n",
        "    reports = df.report.tolist()\n",
        "    preprocessed = [preprocess(report) for report in reports]\n",
        "    df['report'] = preprocessed\n",
        "    df.to_csv('data/df1.csv', index=False)\n",
        "    return df"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8kbhS5K9spd"
      },
      "source": [
        "def to_DF2(df):\n",
        "    '''\n",
        "    Args:\n",
        "      Input : DF1, preprocessed dataframe.\n",
        "              query | report | institute | category\n",
        "      Output: tokenized report from DF1.\n",
        "              index | report\n",
        "                --- | ---\n",
        "              1     | sentence1_token1\n",
        "              1     | sentence1_token2\n",
        "              2     | sentence2_token1\n",
        "    '''\n",
        "    df2 = df.copy()\n",
        "    df2.drop(['query', 'institute', 'category', 'label'], axis=1, inplace=True)\n",
        "    df2.insert(0, 'report_num', pd.factorize(df['report'])[0]+1)\n",
        "    df2.report = [r.split() for r in df.report]\n",
        "    df2 = df2.explode('report')\n",
        "    df2.to_csv('data/df2.csv', index=False)\n",
        "    return df2"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bi0HaTAK93v-"
      },
      "source": [
        "def main():\n",
        "    df = pd.read_csv('lapor_scraping_results.csv')\n",
        "    df1 = to_DF1(df)\n",
        "    to_DF2(df1)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWNHfK9X-L7c"
      },
      "source": [
        "main()"
      ],
      "execution_count": 39,
      "outputs": []
    }
  ]
}
