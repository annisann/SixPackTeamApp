{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lapor_scraping.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPkM6Fs0YJ1R"
      },
      "source": [
        "### Guide\n",
        "query & max page\n",
        "- kebakaran (<60)\n",
        "- tawuran (9)\n",
        "- copet (13)\n",
        "- luka parah (2)\n",
        "- kdrt (4)\n",
        "\n",
        "---\n",
        "**WARNING**: *Must delete existing csv file if you want to run the code with the same query*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOOEWvdtzakZ"
      },
      "source": [
        "## Script structure"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4y8wagXm1F9g"
      },
      "source": [
        "#@title Git Credential { display-mode: \"both\" }\n",
        "#@markdown Exec this cell first\n",
        "username = \"annisann\" #@param {type:\"string\"}\n",
        "password = \"AnnisaNuri10_Github\" #@param {type:\"string\"}\n",
        "name = \"Annisa Nuri Nabila\" #@param {type:\"string\"}\n",
        "email = \"ennoza@student.ub.ac.id\" #@param {type:\"string\"}\n",
        "\n",
        "! git config --global user.name \"{name}\"\n",
        "! git config --global user.username \"{username}\"\n",
        "! git config --global user.email \"{email}\""
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0KbXkuE90_F5",
        "outputId": "5dfaf2b8-f3d7-4327-ceb6-cd78907e5660"
      },
      "source": [
        "! git clone https://{username}:{password}@github.com/briancatraguna/SixPackTeamApp.git && cd SixPackTeamApp/scraping \n",
        "! cd SixPackTeamApp/scraping/ && git remote rm origin\n",
        "! cd SixPackTeamApp/scraping/ && git remote add origin https://{username}:{password}@github.com/briancatraguna/SixPackTeamApp.git\n",
        "! cd SixPackTeamApp/scraping/ && git fetch && git checkout lapor_scraping"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'SixPackTeamApp'...\n",
            "remote: Enumerating objects: 1010, done.\u001b[K\n",
            "remote: Counting objects: 100% (1010/1010), done.\u001b[K\n",
            "remote: Compressing objects: 100% (575/575), done.\u001b[K\n",
            "remote: Total 1010 (delta 436), reused 684 (delta 235), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (1010/1010), 1.04 MiB | 20.08 MiB/s, done.\n",
            "Resolving deltas: 100% (436/436), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tM8dOf0W3c_h",
        "outputId": "ba1eb41e-0659-4be5-9f7c-ce5f40afaeae"
      },
      "source": [
        "#@title Commit the change { vertical-output: true }\n",
        "#@markdown Exec this cell to push change to repository\n",
        "# commit change to dev\n",
        "\n",
        "CommitMessage = \"add kdrt result\" #@param {type:\"string\"}\n",
        "\n",
        "! cd SixPackTeamApp/scraping/data && git add kdrt.csv && git commit -m \"{CommitMessage}\" && git push --force"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[lapor_scraping 4e777ab] add kdrt result\n",
            " 1 file changed, 246 insertions(+)\n",
            " create mode 100644 scraping/data/kdrt.csv\n",
            "Counting objects: 5, done.\n",
            "Delta compression using up to 2 threads.\n",
            "Compressing objects: 100% (5/5), done.\n",
            "Writing objects: 100% (5/5), 7.62 KiB | 7.62 MiB/s, done.\n",
            "Total 5 (delta 3), reused 0 (delta 0)\n",
            "remote: Resolving deltas: 100% (3/3), completed with 3 local objects.\u001b[K\n",
            "To https://github.com/briancatraguna/SixPackTeamApp.git\n",
            "   3ca29e1..4e777ab  lapor_scraping -> lapor_scraping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CdxF7xEmkVR"
      },
      "source": [
        "## Scraping code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMkPsfGinPuw"
      },
      "source": [
        "### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guzIbHKj6AU_"
      },
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import csv\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from bs4 import BeautifulSoup"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLd2gzG4ncwc"
      },
      "source": [
        "### Generate URL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rwc7kwjH1-M2"
      },
      "source": [
        "def get_url(query, page):\n",
        "    \"\"\"\n",
        "    Generate URL from query and page number.\n",
        "    \"\"\"\n",
        "    queryLength = len(query.split())\n",
        "    if queryLength == 1:\n",
        "        url = 'https://www.lapor.go.id/search?q={}&page={}'.format(query, page)\n",
        "    else:\n",
        "        query = query.split()\n",
        "        param = '+'.join(query)\n",
        "        url = 'https://www.lapor.go.id/search?q={}&page={}'.format(param, page)\n",
        "    return url"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98nbosgCne-X"
      },
      "source": [
        "### Generate HTML Source"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_UEgnRv6K8I"
      },
      "source": [
        "def get_html_source(url):\n",
        "    \"\"\"\n",
        "    Send request and log in to the website and return the HTML code.\n",
        "    Replace value of the code below with yours.\n",
        "    See README for more information.\n",
        "    \"\"\"\n",
        "    cookies = {\n",
        "        'lapor': 'eyJpdiI6Ing0MVlmM0RoU1lwSG9ldnA5S2VKS3c9PSIsInZhbHVlIjoiYVlEYUZJMHQyKzV3UnRUSHdiRjVOcGpjZ0YyeWk0WjJzazBmRlo1T1BSSjlGaXRnOFwveEdYRFRRMDREN3dqYUltMUw4cU9FSWRPc0E0MlFFa01SSEJ3PT0iLCJtYWMiOiJiNGJjODhhMzU1OGI0MmQ1NjA2ZGFlYjE3Njc0ZTVhZDEwMmFiYWY2NzJjNmIyZTZiMjkxYzRlNDI3NTNmOWNmIn0%3D',\n",
        "    }\n",
        "\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:88.0) Gecko/20100101 Firefox/88.0',\n",
        "        'Accept': '*/*',\n",
        "        'Accept-Language': 'en-US,en;q=0.5',\n",
        "        'Content-Type': 'application/x-www-form-urlencoded; charset=UTF-8',\n",
        "        'X-OCTOBER-REQUEST-HANDLER': 'laporAuth::onSignin',\n",
        "        'X-OCTOBER-REQUEST-PARTIALS': 'captcha-login',\n",
        "        'X-OCTOBER-REQUEST-FLASH': '1',\n",
        "        'X-Requested-With': 'XMLHttpRequest',\n",
        "        'Origin': 'https://www.lapor.go.id',\n",
        "        'Connection': 'keep-alive',\n",
        "        'Referer': 'https://www.lapor.go.id/',\n",
        "        'Sec-GPC': '1',\n",
        "    }\n",
        "\n",
        "    data = {\n",
        "      '_session_key': 'YOUR_SESSION_KEY',\n",
        "      '_token': 'YOUR_TOKEN',\n",
        "      'login': 'YOUR_EMAIL',\n",
        "      'password': 'YOUR_PASSWORD'\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers, cookies=cookies, timeout=60)\n",
        "        html_code = BeautifulSoup(response.text, 'html.parser')\n",
        "        response = requests.post(url, headers=headers, cookies=cookies, data=data)\n",
        "        return html_code\n",
        "    except Exception:\n",
        "        print('  ERROR: Timeout')"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0sTScOccnpXe"
      },
      "source": [
        "### Scrape the website"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJmLp87GKsrr"
      },
      "source": [
        "def get_report(query, page_len):\n",
        "    \"\"\"\n",
        "    Extract user's reports from the website based on query and page number.\n",
        "    Here, we want to extract reports from user, institute destination, and category of the reports.\n",
        "    In the HTML code, the reports element is in the <p> (paragraph) tag with class attribute `readmore`.\n",
        "    As well as the institute element in the <b> (bold) tag and so on.\n",
        "    \"\"\"\n",
        "    # urls = tqdm([get_url(query, page) for page in range(1, int(page_len)+1)], ncols=100)\n",
        "\n",
        "    url = get_url(query, page_len)\n",
        "\n",
        "    print('   + Getting HTML source . .')\n",
        "    html = get_html_source(url)\n",
        "\n",
        "    pageExist = html.find('p', {'class':'readmore'})\n",
        "    if pageExist:\n",
        "        print('   + Page found! Begin scraping . .')\n",
        "        reports = [(report.text, institute.text, category.text) for report, institute, category in zip(html.find_all('p', {'class':'readmore'}), # report\n",
        "                                                                                                       html.find_all('b'), # institute\n",
        "                                                                                                       html.find_all('a', {'class':'text-muted'}) # category\n",
        "                                                                                                       )\n",
        "        ]\n",
        "        return reports\n",
        "    # for non existing page i.e no reports.\n",
        "    elif pageExist==False:\n",
        "        raise Exception('  ERROR: Page is not exist! Going to the next page ..') # (?)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bs4uEPPnnxI7"
      },
      "source": [
        "### Generate reports DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08lhl7lUdbGy"
      },
      "source": [
        "def generate_dataframe(reports):\n",
        "    \"\"\"\n",
        "    Generate a DataFrame from list of tuples.\n",
        "    The reports output is [(report1, institute1, category1), (report2, institute2, category2), (report3, institute3, category3), ...].\n",
        "    In that case, for each report1, report2, report3 will be added to dictionary, as well as institute1, institute2, institute3.\n",
        "    Then, DataFrame will be created from the dictionary.\n",
        "    \"\"\"\n",
        "    print('   + Generate reports . .')\n",
        "    reportDict = {'report': [],\n",
        "                  'institute': [],\n",
        "                  'category': []}\n",
        "\n",
        "    for report, institute, category in reports:\n",
        "        reportDict['report'].append(report)\n",
        "        reportDict['institute'].append(institute)\n",
        "        reportDict['category'].append(category)\n",
        "        \n",
        "    return pd.DataFrame.from_dict(reportDict)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "siatBh6-n1lf"
      },
      "source": [
        "### Saving csv file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDoIHm1RsaYd"
      },
      "source": [
        "def writeFile(dataframe):\n",
        "    \"\"\"\n",
        "    Save the DataFrame into a csv file.\n",
        "    \"\"\"\n",
        "    print('   + Writing file . .')\n",
        "\n",
        "    DATA_PATH = 'data/'\n",
        "    QUERY_PATH = DATA_PATH + '{}.csv'.format(QUERY)\n",
        "\n",
        "    # if csv file of query is exist\n",
        "    if os.path.exists(QUERY_PATH):\n",
        "        # open the file and will append the new data to the file\n",
        "        with open(QUERY_PATH, 'a+') as file:\n",
        "            dataframe.to_csv(file, header=False, index=False)\n",
        "    \n",
        "    #if data folder doesn't exist\n",
        "    elif os.path.exists(DATA_PATH) == False:\n",
        "        # create folder for data\n",
        "        os.mkdir(DATA_PATH)\n",
        "\n",
        "    # if csv file of query doesn't exist\n",
        "    else:\n",
        "        # write a new file inside the data folder\n",
        "        with open(QUERY_PATH, 'w') as file:\n",
        "            dataframe.to_csv(file, header=True, index=False)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNIBBDqwn6S4"
      },
      "source": [
        "### Main function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wOi8iLIUDh_"
      },
      "source": [
        "def main():\n",
        "    # query = input()\n",
        "    # page_len = str(input())\n",
        "    global QUERY, PAGE_LEN\n",
        "\n",
        "    print('Please insert 1) query, 2) starting page, and 3) max page.')\n",
        "    QUERY = input()\n",
        "    PAGE_START, PAGE_END = str(input()), str(input())\n",
        "\n",
        "    # print('Insert query you\\'d like to scrape.', QUERY)\n",
        "    # print('From what page you\\'d like to scrape?', PAGE_START)\n",
        "    # print('Until what page you\\'d like to scrape?', PAGE_END)\n",
        "\n",
        "    # urls = tqdm([get_url(query, page) for page in range(1, int(page_len)+1)], ncols=100)\n",
        "    print('\\nBegin scraping {} page with keyword \\'{}\\'\\n'.format(int(PAGE_END)-int(PAGE_START), QUERY))\n",
        "\n",
        "    for _num in range(int(PAGE_START), int(PAGE_END)+1):\n",
        "        print('\\noooooooooo PAGE {} oooooooooo'.format(_num))\n",
        "        try:\n",
        "            report = get_report(QUERY, _num)\n",
        "            df = generate_dataframe(report)\n",
        "        except Exception:\n",
        "            # TRY SOLUTION: if '  ERROR: Timeout, continue, else break\n",
        "            print('  ERROR: No result.')\n",
        "            continue # gimana caranya break kalo no result tapi continue kalo timeout\n",
        "        else:\n",
        "            writeFile(df)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_ce3gsoYQs0"
      },
      "source": [
        "main()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}